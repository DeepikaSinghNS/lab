{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "594e07dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************\n",
      "iteration: 0 :::: [[ 0.08580327 -0.91432084]]\n",
      "###output######## [[0.91419673 0.91432084]]\n",
      "**********************\n",
      "iteration: 1 :::: [[ 0.08573216 -0.91336326]]\n",
      "###output######## [[0.91426784 0.91336326]]\n",
      "**********************\n",
      "iteration: 2 :::: [[ 0.08566136 -0.9123876 ]]\n",
      "###output######## [[0.91433864 0.9123876 ]]\n",
      "**********************\n",
      "iteration: 3 :::: [[ 0.08559089 -0.91139341]]\n",
      "###output######## [[0.91440911 0.91139341]]\n",
      "**********************\n",
      "iteration: 4 :::: [[ 0.08552073 -0.91038025]]\n",
      "###output######## [[0.91447927 0.91038025]]\n",
      "**********************\n",
      "iteration: 5 :::: [[ 0.08545088 -0.90934764]]\n",
      "###output######## [[0.91454912 0.90934764]]\n",
      "**********************\n",
      "iteration: 6 :::: [[ 0.08538134 -0.90829511]]\n",
      "###output######## [[0.91461866 0.90829511]]\n",
      "**********************\n",
      "iteration: 7 :::: [[ 0.08531212 -0.90722215]]\n",
      "###output######## [[0.91468788 0.90722215]]\n",
      "**********************\n",
      "iteration: 8 :::: [[ 0.0852432  -0.90612826]]\n",
      "###output######## [[0.9147568  0.90612826]]\n",
      "**********************\n",
      "iteration: 9 :::: [[ 0.0851746  -0.90501292]]\n",
      "###output######## [[0.9148254  0.90501292]]\n",
      "**********************\n",
      "iteration: 10 :::: [[ 0.08510629 -0.9038756 ]]\n",
      "###output######## [[0.91489371 0.9038756 ]]\n",
      "**********************\n",
      "iteration: 11 :::: [[ 0.0850383  -0.90271573]]\n",
      "###output######## [[0.9149617  0.90271573]]\n",
      "**********************\n",
      "iteration: 12 :::: [[ 0.0849706  -0.90153276]]\n",
      "###output######## [[0.9150294  0.90153276]]\n",
      "**********************\n",
      "iteration: 13 :::: [[ 0.08490321 -0.9003261 ]]\n",
      "###output######## [[0.91509679 0.9003261 ]]\n",
      "**********************\n",
      "iteration: 14 :::: [[ 0.08483612 -0.89909515]]\n",
      "###output######## [[0.91516388 0.89909515]]\n",
      "**********************\n",
      "iteration: 15 :::: [[ 0.08476932 -0.89783931]]\n",
      "###output######## [[0.91523068 0.89783931]]\n",
      "**********************\n",
      "iteration: 16 :::: [[ 0.08470281 -0.89655795]]\n",
      "###output######## [[0.91529719 0.89655795]]\n",
      "**********************\n",
      "iteration: 17 :::: [[ 0.0846366  -0.89525041]]\n",
      "###output######## [[0.9153634  0.89525041]]\n",
      "**********************\n",
      "iteration: 18 :::: [[ 0.08457068 -0.89391603]]\n",
      "###output######## [[0.91542932 0.89391603]]\n",
      "**********************\n",
      "iteration: 19 :::: [[ 0.08450505 -0.89255414]]\n",
      "###output######## [[0.91549495 0.89255414]]\n",
      "**********************\n",
      "iteration: 20 :::: [[ 0.0844397  -0.89116404]]\n",
      "###output######## [[0.9155603  0.89116404]]\n",
      "**********************\n",
      "iteration: 21 :::: [[ 0.08437464 -0.88974501]]\n",
      "###output######## [[0.91562536 0.88974501]]\n",
      "**********************\n",
      "iteration: 22 :::: [[ 0.08430985 -0.88829632]]\n",
      "###output######## [[0.91569015 0.88829632]]\n",
      "**********************\n",
      "iteration: 23 :::: [[ 0.08424535 -0.88681721]]\n",
      "###output######## [[0.91575465 0.88681721]]\n",
      "**********************\n",
      "iteration: 24 :::: [[ 0.08418111 -0.88530692]]\n",
      "###output######## [[0.91581889 0.88530692]]\n",
      "**********************\n",
      "iteration: 25 :::: [[ 0.08411715 -0.88376465]]\n",
      "###output######## [[0.91588285 0.88376465]]\n",
      "**********************\n",
      "iteration: 26 :::: [[ 0.08405346 -0.8821896 ]]\n",
      "###output######## [[0.91594654 0.8821896 ]]\n",
      "**********************\n",
      "iteration: 27 :::: [[ 0.08399003 -0.88058095]]\n",
      "###output######## [[0.91600997 0.88058095]]\n",
      "**********************\n",
      "iteration: 28 :::: [[ 0.08392686 -0.87893784]]\n",
      "###output######## [[0.91607314 0.87893784]]\n",
      "**********************\n",
      "iteration: 29 :::: [[ 0.08386396 -0.87725941]]\n",
      "###output######## [[0.91613604 0.87725941]]\n",
      "**********************\n",
      "iteration: 30 :::: [[ 0.0838013  -0.87554477]]\n",
      "###output######## [[0.9161987  0.87554477]]\n",
      "**********************\n",
      "iteration: 31 :::: [[ 0.0837389  -0.87379302]]\n",
      "###output######## [[0.9162611  0.87379302]]\n",
      "**********************\n",
      "iteration: 32 :::: [[ 0.08367674 -0.87200324]]\n",
      "###output######## [[0.91632326 0.87200324]]\n",
      "**********************\n",
      "iteration: 33 :::: [[ 0.08361482 -0.87017449]]\n",
      "###output######## [[0.91638518 0.87017449]]\n",
      "**********************\n",
      "iteration: 34 :::: [[ 0.08355314 -0.8683058 ]]\n",
      "###output######## [[0.91644686 0.8683058 ]]\n",
      "**********************\n",
      "iteration: 35 :::: [[ 0.0834917 -0.8663962]]\n",
      "###output######## [[0.9165083 0.8663962]]\n",
      "**********************\n",
      "iteration: 36 :::: [[ 0.08343048 -0.86444469]]\n",
      "###output######## [[0.91656952 0.86444469]]\n",
      "**********************\n",
      "iteration: 37 :::: [[ 0.08336949 -0.86245025]]\n",
      "###output######## [[0.91663051 0.86245025]]\n",
      "**********************\n",
      "iteration: 38 :::: [[ 0.08330871 -0.86041186]]\n",
      "###output######## [[0.91669129 0.86041186]]\n",
      "**********************\n",
      "iteration: 39 :::: [[ 0.08324815 -0.85832846]]\n",
      "###output######## [[0.91675185 0.85832846]]\n",
      "**********************\n",
      "iteration: 40 :::: [[ 0.08318779 -0.85619901]]\n",
      "###output######## [[0.91681221 0.85619901]]\n",
      "**********************\n",
      "iteration: 41 :::: [[ 0.08312763 -0.8540224 ]]\n",
      "###output######## [[0.91687237 0.8540224 ]]\n",
      "**********************\n",
      "iteration: 42 :::: [[ 0.08306767 -0.85179757]]\n",
      "###output######## [[0.91693233 0.85179757]]\n",
      "**********************\n",
      "iteration: 43 :::: [[ 0.0830079 -0.8495234]]\n",
      "###output######## [[0.9169921 0.8495234]]\n",
      "**********************\n",
      "iteration: 44 :::: [[ 0.08294831 -0.84719878]]\n",
      "###output######## [[0.91705169 0.84719878]]\n",
      "**********************\n",
      "iteration: 45 :::: [[ 0.08288889 -0.84482259]]\n",
      "###output######## [[0.91711111 0.84482259]]\n",
      "**********************\n",
      "iteration: 46 :::: [[ 0.08282964 -0.84239369]]\n",
      "###output######## [[0.91717036 0.84239369]]\n",
      "**********************\n",
      "iteration: 47 :::: [[ 0.08277055 -0.83991096]]\n",
      "###output######## [[0.91722945 0.83991096]]\n",
      "**********************\n",
      "iteration: 48 :::: [[ 0.08271162 -0.83737326]]\n",
      "###output######## [[0.91728838 0.83737326]]\n",
      "**********************\n",
      "iteration: 49 :::: [[ 0.08265282 -0.83477943]]\n",
      "###output######## [[0.91734718 0.83477943]]\n",
      "**********************\n",
      "iteration: 5951 :::: [[ 0.02139198 -0.02263458]]\n",
      "###output######## [[0.97860802 0.02263458]]\n",
      "**********************\n",
      "iteration: 5952 :::: [[ 0.02139023 -0.02263249]]\n",
      "###output######## [[0.97860977 0.02263249]]\n",
      "**********************\n",
      "iteration: 5953 :::: [[ 0.02138847 -0.0226304 ]]\n",
      "###output######## [[0.97861153 0.0226304 ]]\n",
      "**********************\n",
      "iteration: 5954 :::: [[ 0.02138671 -0.02262832]]\n",
      "###output######## [[0.97861329 0.02262832]]\n",
      "**********************\n",
      "iteration: 5955 :::: [[ 0.02138496 -0.02262624]]\n",
      "###output######## [[0.97861504 0.02262624]]\n",
      "**********************\n",
      "iteration: 5956 :::: [[ 0.0213832  -0.02262415]]\n",
      "###output######## [[0.9786168  0.02262415]]\n",
      "**********************\n",
      "iteration: 5957 :::: [[ 0.02138145 -0.02262207]]\n",
      "###output######## [[0.97861855 0.02262207]]\n",
      "**********************\n",
      "iteration: 5958 :::: [[ 0.02137969 -0.02261999]]\n",
      "###output######## [[0.97862031 0.02261999]]\n",
      "**********************\n",
      "iteration: 5959 :::: [[ 0.02137794 -0.0226179 ]]\n",
      "###output######## [[0.97862206 0.0226179 ]]\n",
      "**********************\n",
      "iteration: 5960 :::: [[ 0.02137618 -0.02261582]]\n",
      "###output######## [[0.97862382 0.02261582]]\n",
      "**********************\n",
      "iteration: 5961 :::: [[ 0.02137443 -0.02261374]]\n",
      "###output######## [[0.97862557 0.02261374]]\n",
      "**********************\n",
      "iteration: 5962 :::: [[ 0.02137268 -0.02261166]]\n",
      "###output######## [[0.97862732 0.02261166]]\n",
      "**********************\n",
      "iteration: 5963 :::: [[ 0.02137092 -0.02260958]]\n",
      "###output######## [[0.97862908 0.02260958]]\n",
      "**********************\n",
      "iteration: 5964 :::: [[ 0.02136917 -0.0226075 ]]\n",
      "###output######## [[0.97863083 0.0226075 ]]\n",
      "**********************\n",
      "iteration: 5965 :::: [[ 0.02136742 -0.02260542]]\n",
      "###output######## [[0.97863258 0.02260542]]\n",
      "**********************\n",
      "iteration: 5966 :::: [[ 0.02136567 -0.02260335]]\n",
      "###output######## [[0.97863433 0.02260335]]\n",
      "**********************\n",
      "iteration: 5967 :::: [[ 0.02136392 -0.02260127]]\n",
      "###output######## [[0.97863608 0.02260127]]\n",
      "**********************\n",
      "iteration: 5968 :::: [[ 0.02136217 -0.02259919]]\n",
      "###output######## [[0.97863783 0.02259919]]\n",
      "**********************\n",
      "iteration: 5969 :::: [[ 0.02136042 -0.02259712]]\n",
      "###output######## [[0.97863958 0.02259712]]\n",
      "**********************\n",
      "iteration: 5970 :::: [[ 0.02135867 -0.02259504]]\n",
      "###output######## [[0.97864133 0.02259504]]\n",
      "**********************\n",
      "iteration: 5971 :::: [[ 0.02135692 -0.02259297]]\n",
      "###output######## [[0.97864308 0.02259297]]\n",
      "**********************\n",
      "iteration: 5972 :::: [[ 0.02135517 -0.02259089]]\n",
      "###output######## [[0.97864483 0.02259089]]\n",
      "**********************\n",
      "iteration: 5973 :::: [[ 0.02135342 -0.02258882]]\n",
      "###output######## [[0.97864658 0.02258882]]\n",
      "**********************\n",
      "iteration: 5974 :::: [[ 0.02135167 -0.02258674]]\n",
      "###output######## [[0.97864833 0.02258674]]\n",
      "**********************\n",
      "iteration: 5975 :::: [[ 0.02134992 -0.02258467]]\n",
      "###output######## [[0.97865008 0.02258467]]\n",
      "**********************\n",
      "iteration: 5976 :::: [[ 0.02134817 -0.0225826 ]]\n",
      "###output######## [[0.97865183 0.0225826 ]]\n",
      "**********************\n",
      "iteration: 5977 :::: [[ 0.02134643 -0.02258053]]\n",
      "###output######## [[0.97865357 0.02258053]]\n",
      "**********************\n",
      "iteration: 5978 :::: [[ 0.02134468 -0.02257845]]\n",
      "###output######## [[0.97865532 0.02257845]]\n",
      "**********************\n",
      "iteration: 5979 :::: [[ 0.02134294 -0.02257638]]\n",
      "###output######## [[0.97865706 0.02257638]]\n",
      "**********************\n",
      "iteration: 5980 :::: [[ 0.02134119 -0.02257431]]\n",
      "###output######## [[0.97865881 0.02257431]]\n",
      "**********************\n",
      "iteration: 5981 :::: [[ 0.02133944 -0.02257224]]\n",
      "###output######## [[0.97866056 0.02257224]]\n",
      "**********************\n",
      "iteration: 5982 :::: [[ 0.0213377  -0.02257017]]\n",
      "###output######## [[0.9786623  0.02257017]]\n",
      "**********************\n",
      "iteration: 5983 :::: [[ 0.02133595 -0.02256811]]\n",
      "###output######## [[0.97866405 0.02256811]]\n",
      "**********************\n",
      "iteration: 5984 :::: [[ 0.02133421 -0.02256604]]\n",
      "###output######## [[0.97866579 0.02256604]]\n",
      "**********************\n",
      "iteration: 5985 :::: [[ 0.02133247 -0.02256397]]\n",
      "###output######## [[0.97866753 0.02256397]]\n",
      "**********************\n",
      "iteration: 5986 :::: [[ 0.02133072 -0.0225619 ]]\n",
      "###output######## [[0.97866928 0.0225619 ]]\n",
      "**********************\n",
      "iteration: 5987 :::: [[ 0.02132898 -0.02255984]]\n",
      "###output######## [[0.97867102 0.02255984]]\n",
      "**********************\n",
      "iteration: 5988 :::: [[ 0.02132724 -0.02255777]]\n",
      "###output######## [[0.97867276 0.02255777]]\n",
      "**********************\n",
      "iteration: 5989 :::: [[ 0.0213255  -0.02255571]]\n",
      "###output######## [[0.9786745  0.02255571]]\n",
      "**********************\n",
      "iteration: 5990 :::: [[ 0.02132376 -0.02255364]]\n",
      "###output######## [[0.97867624 0.02255364]]\n",
      "**********************\n",
      "iteration: 5991 :::: [[ 0.02132201 -0.02255158]]\n",
      "###output######## [[0.97867799 0.02255158]]\n",
      "**********************\n",
      "iteration: 5992 :::: [[ 0.02132027 -0.02254951]]\n",
      "###output######## [[0.97867973 0.02254951]]\n",
      "**********************\n",
      "iteration: 5993 :::: [[ 0.02131853 -0.02254745]]\n",
      "###output######## [[0.97868147 0.02254745]]\n",
      "**********************\n",
      "iteration: 5994 :::: [[ 0.02131679 -0.02254539]]\n",
      "###output######## [[0.97868321 0.02254539]]\n",
      "**********************\n",
      "iteration: 5995 :::: [[ 0.02131505 -0.02254333]]\n",
      "###output######## [[0.97868495 0.02254333]]\n",
      "**********************\n",
      "iteration: 5996 :::: [[ 0.02131332 -0.02254126]]\n",
      "###output######## [[0.97868668 0.02254126]]\n",
      "**********************\n",
      "iteration: 5997 :::: [[ 0.02131158 -0.0225392 ]]\n",
      "###output######## [[0.97868842 0.0225392 ]]\n",
      "**********************\n",
      "iteration: 5998 :::: [[ 0.02130984 -0.02253714]]\n",
      "###output######## [[0.97869016 0.02253714]]\n",
      "**********************\n",
      "iteration: 5999 :::: [[ 0.0213081  -0.02253508]]\n",
      "###output######## [[0.9786919  0.02253508]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "inputNeurons=2 \n",
    "hiddenlayerNeurons=4 \n",
    "outputNeurons=2 \n",
    "iteration=6000\n",
    "\n",
    "input = np.random.randint(1,5,inputNeurons) \n",
    "output = np.array([1.0,0.0]) \n",
    "hidden_layer=np.random.rand(1,hiddenlayerNeurons)\n",
    "\n",
    "hidden_biass=np.random.rand(1,hiddenlayerNeurons) \n",
    "output_bias=np.random.rand(1,outputNeurons) \n",
    "hidden_weights=np.random.rand(inputNeurons,hiddenlayerNeurons) \n",
    "output_weights=np.random.rand(hiddenlayerNeurons,outputNeurons)\n",
    "\n",
    "def sigmoid (layer):\n",
    "    return 1/(1 + np.exp(-layer))\n",
    "\n",
    "\n",
    "def gradient(layer): \n",
    "    return layer*(1-layer)\n",
    "\n",
    "for i in range(iteration):\n",
    "\n",
    "    hidden_layer=np.dot(input,hidden_weights) \n",
    "    hidden_layer=sigmoid(hidden_layer+hidden_biass)\n",
    "\n",
    "    output_layer=np.dot(hidden_layer,output_weights) \n",
    "    output_layer=sigmoid(output_layer+output_bias)\n",
    "\n",
    "    error = (output-output_layer) \n",
    "    gradient_outputLayer=gradient(output_layer)\n",
    "    error_terms_output=gradient_outputLayer * error \n",
    "    error_terms_hidden=gradient(hidden_layer)*np.dot(error_terms_output,output_weights.T)\n",
    "\n",
    "    gradient_hidden_weights = np.dot(input.reshape(inputNeurons,1),error_terms_hidden.reshape(1,hiddenlayerNeurons))\n",
    "    gradient_ouput_weights = np.dot(hidden_layer.reshape(hiddenlayerNeurons,1),error_terms_output.reshape(1,outputNeurons))\n",
    "\n",
    "    hidden_weights = hidden_weights + 0.05*gradient_hidden_weights \n",
    "    output_weights = output_weights + 0.05*gradient_ouput_weights \n",
    "    if i<50 or i>iteration-50:\n",
    "        print(\"**********************\") \n",
    "        print(\"iteration:\",i,\"::::\",error) \n",
    "        print(\"###output########\",output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a81d192d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0021761364758430555\n",
      "Loss:0.00017783904229378322\n",
      "Loss:0.00017636948367476515\n",
      "Loss:0.00017499653807884587\n",
      "Loss:0.0001736673287159381\n",
      "Loss:0.0001723799384420727\n",
      "Loss:0.00017113256181995175\n",
      "Loss:0.00016992349451893434\n",
      "Loss:0.0001687511264441002\n",
      "Loss:0.00016761393540416615\n",
      "Loss:0.0001665104812739916\n",
      "Loss:0.00016543940060805267\n",
      "Loss:0.00016439940166543105\n",
      "Loss:0.0001633892598107403\n",
      "Loss:0.00016240781325881074\n",
      "Loss:0.00016145395913402338\n",
      "Loss:0.0001605266498178854\n",
      "Loss:0.00015962488956095528\n",
      "Loss:0.00015874773133740255\n",
      "Loss:0.0001578942739224528\n",
      "Loss:0.00015706365917479145\n",
      "Loss:0.00015625506950761044\n",
      "Loss:0.00015546772553337557\n",
      "Loss:0.00015470088386877797\n",
      "Loss:0.00015395383508744916\n",
      "Loss:0.0001532259018091479\n",
      "Loss:0.0001525164369150457\n",
      "Loss:0.00015182482187964087\n",
      "Loss:0.00015115046521061638\n",
      "Loss:0.00015049280098868806\n",
      "Loss:0.00014985128750015094\n",
      "Loss:0.00014922540595538263\n",
      "Loss:0.00014861465928718697\n",
      "Loss:0.0001480185710232671\n",
      "Loss:0.0001474366842276399\n",
      "Loss:0.00014686856050615023\n",
      "Loss:0.00014631377907170005\n",
      "Loss:0.00014577193586504878\n",
      "Loss:0.00014524264272746708\n",
      "Loss:0.00014472552662169014\n",
      "Loss:0.00014422022889801474\n",
      "Loss:0.00014372640460249175\n",
      "Loss:0.00014324372182451036\n",
      "Loss:0.00014277186108115184\n",
      "Loss:0.00014231051473600118\n",
      "Loss:0.00014185938645018388\n",
      "Loss:0.00014141819066356334\n",
      "Loss:0.0001409866521042443\n",
      "Loss:0.0001405645053246014\n",
      "Loss:0.0001401514942621662\n",
      "Loss:0.00013974737182388726\n",
      "Loss:0.00013935189949231138\n",
      "Loss:0.0001389648469523717\n",
      "Loss:0.0001385859917375382\n",
      "Loss:0.00013821511889421705\n",
      "Loss:0.0001378520206632447\n",
      "Loss:0.00013749649617755786\n",
      "Loss:0.0001371483511750572\n",
      "Loss:0.0001368073977257792\n",
      "Loss:0.0001364734539725972\n",
      "Loss:0.00013614634388462905\n",
      "Loss:0.00013582589702268734\n",
      "Loss:0.00013551194831606074\n",
      "Loss:0.00013520433785001035\n",
      "Loss:0.00013490291066338376\n",
      "Loss:0.0001346075165558261\n",
      "Loss:0.00013431800990397044\n",
      "Loss:0.0001340342494862713\n",
      "Loss:0.00013375609831586444\n",
      "Loss:0.0001334834234811369\n",
      "Loss:0.00013321609599352752\n",
      "Loss:0.0001329539906422382\n",
      "Loss:0.00013269698585543406\n",
      "Loss:0.0001324449635676416\n",
      "Loss:0.00013219780909300437\n",
      "Loss:0.00013195541100410955\n",
      "Loss:0.00013171766101608386\n",
      "Loss:0.0001314844538756982\n",
      "Loss:0.0001312556872552604\n",
      "Loss:0.00013103126165099296\n",
      "Loss:0.00013081108028574508\n",
      "Loss:0.00013059504901578148\n",
      "Loss:0.00013038307624144003\n",
      "Loss:0.00013017507282152608\n",
      "Loss:0.00012997095199119497\n",
      "Loss:0.0001297706292831975\n",
      "Loss:0.0001295740224523355\n",
      "Loss:0.00012938105140291318\n",
      "Loss:0.00012919163811913857\n",
      "Loss:0.00012900570659824673\n",
      "Loss:0.00012882318278627163\n",
      "Loss:0.00012864399451634044\n",
      "Loss:0.0001284680714493393\n",
      "Loss:0.00012829534501689044\n",
      "Loss:0.0001281257483664831\n",
      "Loss:0.00012795921630872527\n",
      "Loss:0.0001277956852665646\n",
      "Loss:0.0001276350932264022\n",
      "Loss:0.00012747737969104531\n",
      "Loss:0.00012732248563439581\n",
      "Loss:0.00012717035345778254\n",
      "Loss:0.00012702092694789899\n",
      "Loss:0.00012687415123627035\n",
      "Loss:0.00012672997276013713\n",
      "Loss:0.00012658833922479275\n",
      "Loss:0.000126449199567191\n",
      "Loss:0.00012631250392088335\n",
      "Loss:0.0001261782035821563\n",
      "Loss:0.00012604625097733138\n",
      "Loss:0.00012591659963122706\n",
      "Loss:0.00012578920413665865\n",
      "Loss:0.0001256640201250033\n",
      "Loss:0.0001255410042377301\n",
      "Loss:0.0001254201140989135\n",
      "Loss:0.00012530130828863914\n",
      "Loss:0.00012518454631728554\n",
      "Loss:0.00012506978860066288\n",
      "Loss:0.00012495699643594942\n",
      "Loss:0.00012484613197842945\n",
      "Loss:0.00012473715821892433\n",
      "Loss:0.00012463003896200997\n",
      "Loss:0.00012452473880488155\n",
      "Loss:0.0001244212231168954\n",
      "Loss:0.00012431945801974473\n",
      "Loss:0.00012421941036826903\n",
      "Loss:0.00012412104773182393\n",
      "Loss:0.00012402433837626278\n",
      "Loss:0.00012392925124641975\n",
      "Loss:0.00012383575594916637\n",
      "Loss:0.00012374382273694972\n",
      "Loss:0.00012365342249182244\n",
      "Loss:0.00012356452670997164\n",
      "Loss:0.0001234771074866666\n",
      "Loss:0.00012339113750168377\n",
      "Loss:0.000123306590005123\n",
      "Loss:0.00012322343880366514\n",
      "Loss:0.00012314165824718775\n",
      "Loss:0.00012306122321581022\n",
      "Loss:0.00012298210910725998\n",
      "Loss:0.00012290429182462917\n",
      "Loss:0.0001228277477644449\n",
      "Loss:0.00012275245380511315\n",
      "Loss:0.00012267838729563345\n",
      "Loss:0.00012260552604466637\n",
      "Loss:0.0001225338483098767\n",
      "Loss:0.00012246333278757374\n",
      "Loss:0.0001223939586026331\n",
      "Loss:0.00012232570529869163\n",
      "Loss:0.00012225855282860246\n",
      "Loss:0.00012219248154513544\n",
      "Loss:0.0001221274721919448\n",
      "Loss:0.00012206350589475492\n",
      "Loss:0.00012200056415279011\n",
      "Loss:0.00012193862883041767\n",
      "Loss:0.00012187768214901456\n",
      "Loss:0.00012181770667904855\n",
      "Loss:0.00012175868533234434\n",
      "Loss:0.00012170060135456531\n",
      "Loss:0.00012164343831788622\n",
      "Loss:0.00012158718011383604\n",
      "Loss:0.00012153181094633057\n",
      "Loss:0.00012147731532488545\n",
      "Loss:0.00012142367805799218\n",
      "Loss:0.00012137088424664816\n",
      "Loss:0.00012131891927807156\n",
      "Loss:0.00012126776881954465\n",
      "Loss:0.00012121741881242281\n",
      "Loss:0.00012116785546629408\n",
      "Loss:0.00012111906525325754\n",
      "Loss:0.00012107103490237231\n",
      "Loss:0.00012102375139420483\n",
      "Loss:0.00012097720195554177\n",
      "Loss:0.00012093137405420167\n",
      "Loss:0.00012088625539398341\n",
      "Loss:0.0001208418339097365\n",
      "Loss:0.00012079809776253425\n",
      "Loss:0.00012075503533497403\n",
      "Loss:0.00012071263522657856\n",
      "Loss:0.00012067088624932053\n",
      "Loss:0.00012062977742321829\n",
      "Loss:0.00012058929797206656\n",
      "Loss:0.00012054943731925484\n",
      "Loss:0.00012051018508368152\n",
      "Loss:0.00012047153107574796\n",
      "Loss:0.00012043346529346662\n",
      "Loss:0.00012039597791864708\n",
      "Loss:0.00012035905931316562\n",
      "Loss:0.0001203227000153242\n",
      "Loss:0.0001202868907362906\n",
      "Loss:0.00012025162235661353\n",
      "Loss:0.00012021688592282477\n",
      "Loss:0.00012018267264411336\n",
      "Loss:0.00012014897388906174\n",
      "Loss:0.00012011578118248258\n",
      "Loss:0.00012008308620228642\n",
      "Loss:0.00012005088077646715\n",
      "Loss:0.00012001915688009854\n",
      "Loss:0.00011998790663244394\n",
      "Loss:0.00011995712229410676\n",
      "Loss:0.00011992679626423638\n",
      "Loss:0.00011989692107781762\n",
      "Loss:0.00011986748940300235\n",
      "Loss:0.00011983849403849467\n",
      "Loss:0.00011980992791101172\n",
      "Loss:0.00011978178407278442\n",
      "Loss:0.00011975405569910863\n",
      "Loss:0.00011972673608596467\n",
      "Loss:0.00011969981864767045\n",
      "Loss:0.00011967329691460021\n",
      "Loss:0.00011964716453093804\n",
      "Loss:0.00011962141525248917\n",
      "Loss:0.00011959604294453001\n",
      "Loss:0.00011957104157971092\n",
      "Loss:0.00011954640523598956\n",
      "Loss:0.00011952212809463203\n",
      "Loss:0.00011949820443823266\n",
      "Loss:0.00011947462864877368\n",
      "Loss:0.00011945139520576498\n",
      "Loss:0.00011942849868435599\n",
      "Loss:0.00011940593375355032\n",
      "Loss:0.00011938369517442142\n",
      "Loss:0.00011936177779837574\n",
      "Loss:0.00011934017656544547\n",
      "Loss:0.00011931888650263015\n",
      "Loss:0.0001192979027222583\n",
      "Loss:0.00011927722042039033\n",
      "Loss:0.00011925683487524759\n",
      "Loss:0.00011923674144568761\n",
      "Loss:0.00011921693556968506\n",
      "Loss:0.00011919741276287415\n",
      "Loss:0.00011917816861709566\n",
      "Loss:0.00011915919879899287\n",
      "Loss:0.000119140499048616\n",
      "Loss:0.00011912206517807077\n",
      "Loss:0.0001191038930701871\n",
      "Loss:0.00011908597867722034\n",
      "Loss:0.00011906831801956635\n",
      "Loss:0.00011905090718451979\n",
      "Loss:0.00011903374232503947\n",
      "Loss:0.0001190168196585506\n",
      "Loss:0.00011900013546576741\n",
      "Loss:0.0001189836860895392\n",
      "Loss:0.00011896746793371557\n",
      "Loss:0.00011895147746203995\n",
      "Loss:0.00011893571119706502\n",
      "Loss:0.00011892016571907813\n",
      "Loss:0.00011890483766506809\n",
      "Loss:0.00011888972372769165\n",
      "Loss:0.00011887482065428311\n",
      "Loss:0.00011886012524584608\n",
      "Loss:0.00011884563435611696\n",
      "Loss:0.00011883134489059658\n",
      "Loss:0.00011881725380563829\n",
      "Loss:0.00011880335810752388\n",
      "Loss:0.00011878965485158166\n",
      "Loss:0.00011877614114131535\n",
      "Loss:0.0001187628141275311\n",
      "Loss:0.00011874967100751433\n",
      "Loss:0.00011873670902419242\n",
      "Loss:0.00011872392546533007\n",
      "Loss:0.00011871131766273803\n",
      "Loss:0.00011869888299149093\n",
      "Loss:0.0001186866188691674\n",
      "Loss:0.00011867452275510188\n",
      "Loss:0.00011866259214965137\n",
      "Loss:0.0001186508245934662\n",
      "Loss:0.00011863921766679542\n",
      "Loss:0.00011862776898879101\n",
      "Loss:0.00011861647621682173\n",
      "Loss:0.00011860533704581138\n",
      "Loss:0.00011859434920758815\n",
      "Loss:0.0001185835104702333\n",
      "Loss:0.00011857281863745812\n",
      "Loss:0.00011856227154799249\n",
      "Loss:0.00011855186707495392\n",
      "Loss:0.0001185416031252855\n",
      "Loss:0.00011853147763914818\n",
      "Loss:0.00011852148858935948\n",
      "Loss:0.00011851163398082495\n",
      "Loss:0.00011850191184999428\n",
      "Loss:0.00011849232026431157\n",
      "Loss:0.00011848285732169162\n",
      "Loss:0.00011847352114999195\n",
      "Loss:0.00011846430990650533\n",
      "Loss:0.00011845522177745945\n",
      "Loss:0.00011844625497751857\n",
      "Loss:0.00011843740774930538\n",
      "Loss:0.00011842867836291815\n",
      "Loss:0.00011842006511548208\n",
      "Loss:0.00011841156633066181\n",
      "Loss:0.00011840318035824754\n",
      "Loss:0.00011839490557368836\n",
      "Loss:0.0001183867403776683\n",
      "Loss:0.00011837868319568543\n",
      "Loss:0.00011837073247762484\n",
      "Loss:0.00011836288669735942\n",
      "Loss:0.00011835514435234598\n",
      "Loss:0.00011834750396321821\n",
      "Loss:0.00011833996407341834\n",
      "Loss:0.00011833252324880337\n",
      "Loss:0.00011832518007727417\n",
      "Loss:0.00011831793316841224\n",
      "Loss:0.00011831078115311533\n",
      "Loss:0.00011830372268324304\n",
      "Loss:0.00011829675643127239\n",
      "Loss:0.00011828988108996228\n",
      "Loss:0.00011828309537199886\n",
      "Loss:0.00011827639800968805\n",
      "Loss:0.00011826978775462308\n",
      "Loss:0.00011826326337736622\n",
      "Loss:0.00011825682366713787\n",
      "Loss:0.00011825046743151025\n",
      "Loss:0.00011824419349610731\n",
      "Loss:0.00011823800070431194\n",
      "Loss:0.0001182318879169707\n",
      "Loss:0.00011822585401211285\n",
      "Loss:0.00011821989788466271\n",
      "Loss:0.00011821401844617959\n",
      "Loss:0.00011820821462456786\n",
      "Loss:0.0001182024853638307\n",
      "Loss:0.00011819682962379705\n",
      "Loss:0.00011819124637986656\n",
      "Loss:0.0001181857346227662\n",
      "Loss:0.00011818029335828769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.00011817492160705984\n",
      "Loss:0.00011816961840430525\n",
      "Loss:0.00011816438279959762\n",
      "Loss:0.00011815921385664164\n",
      "Loss:0.00011815411065304041\n",
      "Loss:0.00011814907228007653\n",
      "Loss:0.00011814409784248823\n",
      "Loss:0.00011813918645826138\n",
      "Loss:0.00011813433725840972\n",
      "Loss:0.00011812954938677757\n",
      "Loss:0.00011812482199982443\n",
      "Loss:0.00011812015426643467\n",
      "Loss:0.0001181155453677151\n",
      "Loss:0.00011811099449679707\n",
      "Loss:0.00011810650085865915\n",
      "Loss:0.00011810206366992482\n",
      "Loss:0.00011809768215869033\n",
      "Loss:0.00011809335556433534\n",
      "Loss:0.00011808908313734877\n",
      "Loss:0.00011808486413916325\n",
      "Loss:0.00011808069784195838\n",
      "Loss:0.00011807658352852454\n",
      "Loss:0.00011807252049206713\n",
      "Loss:0.00011806850803606658\n",
      "Loss:0.00011806454547410077\n",
      "Loss:0.00011806063212969583\n",
      "Loss:0.0001180567673361758\n",
      "Loss:0.00011805295043649258\n",
      "Loss:0.00011804918078309934\n",
      "Loss:0.00011804545773778486\n",
      "Loss:0.00011804178067153625\n",
      "Loss:0.00011803814896439592\n",
      "Loss:0.00011803456200532363\n",
      "Loss:0.00011803101919204976\n",
      "Loss:0.00011802751993095165\n",
      "Loss:0.00011802406363691267\n",
      "Loss:0.00011802064973319433\n",
      "Loss:0.000118017277651307\n",
      "Loss:0.00011801394683087925\n",
      "Loss:0.00011801065671954257\n",
      "Loss:0.00011800740677279844\n",
      "Loss:0.00011800419645390644\n",
      "Loss:0.00011800102523375746\n",
      "Loss:0.00011799789259076455\n",
      "Loss:0.00011799479801074517\n",
      "Loss:0.00011799174098680664\n",
      "Loss:0.00011798872101923795\n",
      "Loss:0.00011798573761540077\n",
      "Loss:0.00011798279028962104\n",
      "Loss:0.00011797987856308317\n",
      "Loss:0.00011797700196372307\n",
      "Loss:0.00011797416002613341\n",
      "Loss:0.0001179713522914621\n",
      "Loss:0.00011796857830730307\n",
      "Loss:0.00011796583762761134\n",
      "Loss:0.0001179631298126006\n",
      "Loss:0.00011796045442865234\n",
      "Loss:0.0001179578110482223\n",
      "Loss:0.00011795519924974659\n",
      "Loss:0.00011795261861755574\n",
      "Loss:0.00011795006874178579\n",
      "Loss:0.00011794754921829222\n",
      "Loss:0.00011794505964855789\n",
      "Loss:0.00011794259963962156\n",
      "Loss:0.00011794016880398346\n",
      "Loss:0.00011793776675952915\n",
      "Loss:0.00011793539312944892\n",
      "Loss:0.0001179330475421605\n",
      "Loss:0.00011793072963122916\n",
      "Loss:0.00011792843903529302\n",
      "Loss:0.0001179261753979871\n",
      "Loss:0.00011792393836786718\n",
      "Loss:0.0001179217275983433\n",
      "Loss:0.00011791954274760404\n",
      "Loss:0.0001179173834785442\n",
      "Loss:0.00011791524945870056\n",
      "Loss:0.00011791314036017918\n",
      "Loss:0.00011791105585959065\n",
      "Loss:0.00011790899563798373\n",
      "Loss:0.00011790695938078443\n",
      "Loss:0.0001179049467777189\n",
      "Loss:0.00011790295752276477\n",
      "Loss:0.0001179009913140848\n",
      "Loss:0.00011789904785396079\n",
      "Loss:0.000117897126848741\n",
      "Loss:0.0001178952280087728\n",
      "Loss:0.00011789335104835476\n",
      "Loss:0.0001178914956856716\n",
      "Loss:0.0001178896616427336\n",
      "Loss:0.00011788784864533924\n",
      "Loss:0.00011788605642299647\n",
      "Loss:0.00011788428470888957\n",
      "Loss:0.00011788253323981081\n",
      "Loss:0.00011788080175612269\n",
      "Loss:0.00011787909000169167\n",
      "Loss:0.00011787739772384963\n",
      "Loss:0.00011787572467333466\n",
      "Loss:0.00011787407060425017\n",
      "Loss:0.00011787243527401065\n",
      "Loss:0.00011787081844329734\n",
      "Loss:0.0001178692198760136\n",
      "Loss:0.00011786763933923142\n",
      "Loss:0.00011786607660315363\n",
      "Loss:0.0001178645314410631\n",
      "Loss:0.00011786300362928134\n",
      "Loss:0.00011786149294713015\n",
      "Loss:0.00011785999917687891\n",
      "Loss:0.00011785852210371169\n",
      "Loss:0.00011785706151567837\n",
      "Loss:0.00011785561720366474\n",
      "Loss:0.0001178541889613361\n",
      "Loss:0.00011785277658510953\n",
      "Loss:0.00011785137987411688\n",
      "Loss:0.00011784999863015908\n",
      "Loss:0.00011784863265766858\n",
      "Loss:0.00011784728176367894\n",
      "Loss:0.0001178459457577802\n",
      "Loss:0.00011784462445209038\n",
      "Loss:0.00011784331766121176\n",
      "Loss:0.0001178420252022056\n",
      "Loss:0.00011784074689454741\n",
      "Loss:0.00011783948256009813\n",
      "Loss:0.00011783823202307719\n",
      "Loss:0.00011783699511001106\n",
      "Loss:0.00011783577164972772\n",
      "Loss:0.00011783456147329632\n",
      "Loss:0.00011783336441401251\n",
      "Loss:0.00011783218030736785\n",
      "Loss:0.00011783100899101327\n",
      "Loss:0.00011782985030472482\n",
      "Loss:0.00011782870409039348\n",
      "Loss:0.0001178275701919672\n",
      "Loss:0.00011782644845544955\n",
      "Loss:0.00011782533872885161\n",
      "Loss:0.00011782424086217813\n",
      "Loss:0.00011782315470738467\n",
      "Loss:0.00011782208011837049\n",
      "Loss:0.00011782101695093144\n",
      "Loss:0.00011781996506274625\n",
      "Loss:0.00011781892431335015\n",
      "Loss:0.00011781789456410102\n",
      "Loss:0.00011781687567816291\n",
      "Loss:0.0001178158675204807\n",
      "Loss:0.00011781486995774451\n",
      "Loss:0.00011781388285838165\n",
      "Loss:0.00011781290609251946\n",
      "Loss:0.00011781193953197512\n",
      "Loss:0.00011781098305021656\n",
      "Loss:0.00011781003652235457\n",
      "Loss:0.00011780909982511039\n",
      "Loss:0.00011780817283680157\n",
      "Loss:0.00011780725543731154\n",
      "Loss:0.00011780634750807302\n",
      "Loss:0.00011780544893205355\n",
      "Loss:0.00011780455959371414\n",
      "Loss:0.00011780367937901782\n",
      "Loss:0.00011780280817538303\n",
      "Loss:0.00011780194587167728\n",
      "Loss:0.00011780109235820135\n",
      "Loss:0.00011780024752665238\n",
      "Loss:0.00011779941127012363\n",
      "Loss:0.00011779858348307596\n",
      "Loss:0.00011779776406131954\n",
      "Loss:0.00011779695290200236\n",
      "Loss:0.00011779614990358075\n",
      "Loss:0.00011779535496581277\n",
      "Loss:0.00011779456798973721\n",
      "Loss:0.00011779378887764646\n",
      "Loss:0.00011779301753308891\n",
      "Loss:0.00011779225386083493\n",
      "Loss:0.00011779149776687048\n",
      "Loss:0.0001177907491583774\n",
      "Loss:0.00011779000794371026\n",
      "Loss:0.0001177892740324011\n",
      "Loss:0.00011778854733511822\n",
      "Loss:0.00011778782776366729\n",
      "Input:[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output:[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output:[[0.90552185]\n",
      " [0.87192923]\n",
      " [0.89119923]]\n",
      "Loss:0.00011778711523097436\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array(([2,9],[1,5],[3,6]), dtype=float)\n",
    "y = np.array(([92],[86],[89]), dtype=float)\n",
    "\n",
    "X = X/np.amax(X,axis=0)\n",
    "y = y/100\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self):\n",
    "        self.inputsize = 2\n",
    "        self.outputsize = 1\n",
    "        self.hiddensize = 3\n",
    "        self.w1 = np.random.rand(self.inputsize, self.hiddensize)\n",
    "        self.w2 = np.random.rand(self.hiddensize, self.outputsize)\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        self.z = np.dot(X, self.w1)\n",
    "        self.z2 = self.sigmoid(self.z)\n",
    "        self.z3 = np.dot(self.z2, self.w2)\n",
    "        output = self.sigmoid(self.z3)\n",
    "        return output\n",
    "    \n",
    "    def sigmoid(self,s,deriv=False):\n",
    "        if deriv == True:\n",
    "            return s*(1-s)\n",
    "        return 1/(1+np.exp(-s))\n",
    "    \n",
    "    def backward(self,X,y,output):\n",
    "        self.output_error = y-output\n",
    "        self.output_delta = self.output_error*self.sigmoid(output,deriv=True)\n",
    "        self.z2_error = self.output_delta.dot(self.w2.T)\n",
    "        self.z2_delta = self.z2_error*self.sigmoid(self.z2, deriv=True)\n",
    "        self.w1 += X.T.dot(self.z2_delta)\n",
    "        self.w2 += self.z2.T.dot(self.output_delta)\n",
    "\n",
    "    def train(self,X,y):\n",
    "        output = self.feedforward(X)\n",
    "        self.backward(X,y,output)\n",
    "\n",
    "NN = NeuralNetwork()\n",
    "for i in range(50000):\n",
    "    if (i%100 == 0):\n",
    "        print(\"Loss:\"+str(np.mean(np.square(y-NN.feedforward(X)))))\n",
    "    NN.train(X,y)\n",
    "print(\"Input:\"+str(X))\n",
    "print(\"Actual Output:\"+str(y))\n",
    "print(\"Predicted Output:\"+str(NN.feedforward(X)))\n",
    "print(\"Loss:\"+str(np.mean(np.square(y-NN.feedforward(X)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363b76e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
